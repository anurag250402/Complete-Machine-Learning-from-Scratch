{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35ba129",
   "metadata": {},
   "source": [
    "# Classification Metrics (Confusion Matrix, ROC-AUC, PR Curve, F1)\n",
    "\n",
    "This notebook explains important classification metrics: **Confusion Matrix, ROC-AUC, Precision-Recall Curve, and F1 Score**. We'll demonstrate each using a simple classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ae41a7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4257d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, precision_recall_curve, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406b10b",
   "metadata": {},
   "source": [
    "## 2. Load Example Dataset\n",
    "We'll use the breast cancer dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('Train shape:', X_train.shape)\n",
    "print('Test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b678594",
   "metadata": {},
   "source": [
    "## 3. Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b4281",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix\n",
    "A confusion matrix shows counts of true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb6ea2",
   "metadata": {},
   "source": [
    "## 5. ROC Curve and AUC\n",
    "**ROC Curve** plots True Positive Rate vs. False Positive Rate. **AUC** (Area Under Curve) summarizes the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d690882",
   "metadata": {},
   "source": [
    "## 6. Precision-Recall (PR) Curve\n",
    "The PR curve shows the tradeoff between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390bff1",
   "metadata": {},
   "source": [
    "## 7. F1 Score\n",
    "**F1 Score** is the harmonic mean of precision and recall.\n",
    "\\[\n",
    "F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a889a",
   "metadata": {},
   "source": [
    "## 8. Classification Report\n",
    "A summary including precision, recall, f1-score, and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba089fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037848d",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "- **Confusion Matrix**: Breakdown of predictions vs. reality.\n",
    "- **ROC-AUC**: Quality of separation between classes.\n",
    "- **PR Curve**: Precision/recall trade-off.\n",
    "- **F1 Score**: Balance of precision and recall."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
