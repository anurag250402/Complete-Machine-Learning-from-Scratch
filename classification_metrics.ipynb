{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Classification Metrics\n\nIn this notebook, we will explore various classification metrics that help us evaluate the performance of machine learning models, such as Confusion Matrix, ROC-AUC, PR Curve, and F1 Score."}, {"cell_type": "markdown", "metadata": {}, "source": "## Confusion Matrix\n\nA confusion matrix is a table used to describe the performance of a classification model. It shows the true positive, false positive, true negative, and false negative values."}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 1: Import Required Libraries for Classification Metrics\n\nWe will begin by importing necessary libraries such as `confusion_matrix`, `roc_curve`, `auc`, and `precision_recall_curve` from `sklearn`."}, {"cell_type": "code", "metadata": {}, "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, f1_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 2: Create a Sample Dataset\n\nWe will generate a synthetic dataset for binary classification."}, {"cell_type": "code", "metadata": {}, "source": "# Generating synthetic binary classification dataset\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_classes=2, random_state=42)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 3: Train a Classifier\n\nNow, we will train a logistic regression model on the generated dataset."}, {"cell_type": "code", "metadata": {}, "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 4: Confusion Matrix\n\nWe will compute the confusion matrix to evaluate the model's performance."}, {"cell_type": "code", "metadata": {}, "source": "y_pred = model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:')\nprint(conf_matrix)"}, {"cell_type": "markdown", "metadata": {}, "source": "## ROC-AUC\n\nThe ROC (Receiver Operating Characteristic) curve illustrates the trade-off between the true positive rate (sensitivity) and false positive rate (1-specificity) at various thresholds. The ROC-AUC score is the area under this curve."}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 5: ROC-AUC\n\nWe will now calculate and plot the ROC curve, as well as compute the AUC score."}, {"cell_type": "code", "metadata": {}, "source": "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Precision-Recall Curve (PR Curve)\n\nThe Precision-Recall curve is used to evaluate models in the case of imbalanced classes. It shows the trade-off between precision and recall for different thresholds."}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 6: PR Curve\n\nWe will now calculate and plot the Precision-Recall curve."}, {"cell_type": "code", "metadata": {}, "source": "precision, recall, thresholds_pr = precision_recall_curve(y_test, model.predict_proba(X_test)[:, 1])\nplt.plot(recall, precision, color='green')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## F1 Score\n\nThe F1 score is the harmonic mean of precision and recall. It provides a balance between these two metrics and is useful when we have imbalanced datasets."}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 7: F1 Score\n\nWe will now calculate the F1 score for the model."}, {"cell_type": "code", "metadata": {}, "source": "f1 = f1_score(y_test, y_pred)\nprint(f'F1 Score: {f1:.2f}')"}, {"cell_type": "markdown", "metadata": {}, "source": "## Conclusion\n\nIn this notebook, we have explored various classification metrics, including Confusion Matrix, ROC-AUC, PR Curve, and F1 Score. These metrics provide valuable insights into the performance of classification models."}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}